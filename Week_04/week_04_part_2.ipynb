{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aglqzo3canlF"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "<h1>Notebook: [ Week #04: Building your own RAG Bot ]</h1>\n",
        "\n",
        "- Your objective in this notebook is create a RAG Bot that allow the users to interact with some notes from AI Champions Bootcamp.\n",
        "- A convenient way to work on this notebook is to open the earlier Jupyter Notebook in `Topic 4`. Yes, the notebook with pre-populated code cells.\n",
        "- You can refer to how a simple RAG Bot (or more like a RAG pipeline) is built\n",
        "- You may extend the functionalities of the bot as you wish.\n",
        "- Minimumly, you should have a simple RAG Bot like the one in the earlier `Topic 4` Jupyter Notebook\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyGh4T0wanlG"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxGrf5weanlG"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install langchain\n",
        "!pip install langchain-openai\n",
        "!pip install langchain-experimental\n",
        "!pip install langchain-chroma\n",
        "!pip install pypdf\n",
        "!pip install lolviz\n",
        "!pip install chromadb\n",
        "!pip install tqdm\n",
        "!pip install tiktoken\n",
        "\n",
        "# You may need to install other dependencies that you need for your project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnQDqT9eanlG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from getpass import getpass\n",
        "\n",
        "# Set up the OpenAI API key by setting the OPENAI_API_KEY environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API Key\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIqKLyvhanlG"
      },
      "source": [
        "---\n",
        "\n",
        "## Helper Functions\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7EpoVjoanlH"
      },
      "source": [
        "### Function for Generating Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4BHqlqjanlH"
      },
      "outputs": [],
      "source": [
        "def get_embedding(input, model='text-embedding-3-small'):\n",
        "    response = client.embeddings.create(\n",
        "        input=input,\n",
        "        model=model\n",
        "    )\n",
        "    return [x.embedding for x in response.data]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRc55ndeanlH"
      },
      "source": [
        "### Function for Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rozhdpLkanlH"
      },
      "outputs": [],
      "source": [
        "# This is the \"Updated\" helper function for calling LLM\n",
        "def get_completion(prompt, model=\"gpt-4o-mini\", temperature=0, top_p=1.0, max_tokens=256, n=1, json_output=False):\n",
        "    if json_output == True:\n",
        "      output_json_structure = {\"type\": \"json_object\"}\n",
        "    else:\n",
        "      output_json_structure = None\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = client.chat.completions.create( #originally was openai.chat.completions\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        max_tokens=max_tokens,\n",
        "        n=1,\n",
        "        response_format=output_json_structure,\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZ7mi2G0anlH"
      },
      "outputs": [],
      "source": [
        "# This a \"modified\" helper function that we will discuss in this session\n",
        "# Note that this function directly take in \"messages\" as the parameter.\n",
        "def get_completion_by_messages(messages, model=\"gpt-4o-mini\", temperature=0, top_p=1.0, max_tokens=1024, n=1):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        max_tokens=max_tokens,\n",
        "        n=1\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm9X1jwEanlH"
      },
      "source": [
        "## Functions for Token Counting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jek79p3oanlH"
      },
      "outputs": [],
      "source": [
        "# These functions are for calculating the tokens.\n",
        "# âš ï¸ These are simplified implementations that are good enough for a rough estimation.\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "def count_tokens(text):\n",
        "    encoding = tiktoken.encoding_for_model('gpt-4o-mini')\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "def count_tokens_from_message(messages):\n",
        "    encoding = tiktoken.encoding_for_model('gpt-4o-mini')\n",
        "    value = ' '.join([x.get('content') for x in messages])\n",
        "    return len(encoding.encode(value))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23QgT89NanlH"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "# Create a \"Chat with your Document\" Bot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukcr0XcHanlH"
      },
      "source": [
        "**\\[ Overview of Steps in RAG \\]**\n",
        "\n",
        "- 1. **Document Loading**\n",
        "\t- In this initial step, relevant documents are ingested and prepared for further processing. This process typically occurs offline.\n",
        "- 2. **Splitting & Chunking**\n",
        "\t- The text from the documents is split into smaller chunks or segments.\n",
        "\t- These chunks serve as the building blocks for subsequent stages.\n",
        "- 3. **Storage**\n",
        "\t- The embeddings (vector representations) of these chunks are created and stored in a vector store.\n",
        "\t- These embeddings capture the semantic meaning of the text.\n",
        "- 4. **Retrieval**\n",
        "\t- When an online query arrives, the system retrieves relevant chunks from the vector store based on the query.\n",
        "\t- This retrieval step ensures that the system identifies the most pertinent information.\n",
        "- 5. **Output**\n",
        "\t- Finally, the retrieved chunks are used to generate a coherent response.\n",
        "\t- This output can be in the form of natural language text, summaries, or other relevant content.\n",
        "\n",
        "![](https://abc-notes.data.tech.gov.sg/resources/img/topic-4-rag-overview.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "So9NhIylanlH"
      },
      "source": [
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdjYmsyLanlH"
      },
      "source": [
        "## Document Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l499JwUWanlI"
      },
      "source": [
        "Here are the \"notes\" that you must include in your RAG pipeline as the `Documents`\n",
        "- [Key Parameters for LLMs](https://abc-notes.data.tech.gov.sg/notes/topic-2-deeper-dive-into-llms/2.-key-parameters-for-llms.html)\n",
        "- [LLMs and Hallucinations](https://abc-notes.data.tech.gov.sg/notes/topic-2-deeper-dive-into-llms/3.-llms-and-hallucinations.html)\n",
        "- [Prompting Techniques for BUilders](https://abc-notes.data.tech.gov.sg/notes/topic-2-deeper-dive-into-llms/4.-prompting-techniques-for-builders.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHhmnnFLanlI"
      },
      "source": [
        "You have three options.\n",
        "1) ðŸ’ªðŸ¼ Take up the challenge to find a way to get the content directly from the webpages above.\n",
        "2) ðŸ¥´ Go with the easy route, download the notes nicely prepared in a `.txt` format. Download the zipped file [here](https://abc-notes.data.tech.gov.sg/resources/data/notes.zip)\n",
        "3) ðŸ˜Ž â€œOnly children choose; adults take all.â€ Experiment with both data sources and see which can help to the Bot to provide more accurate information for the user queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Trmee40xanlI"
      },
      "source": [
        "---\n",
        "\n",
        "> ðŸ’¡ **Feel free to add as many code cells as your need.**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MReaOxLZanlI"
      },
      "outputs": [],
      "source": [
        "# < Your Code Here >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV0Smp3hanlI"
      },
      "source": [
        "## Splitting & Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8ZH2ToWanlI"
      },
      "outputs": [],
      "source": [
        "# < Your Code Here >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEEg65AXanlI"
      },
      "source": [
        "## Storage: Embedding & Vectorstores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1O1y1GCdanlI"
      },
      "outputs": [],
      "source": [
        "# < Your Code Here >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-dLIQrianlI"
      },
      "source": [
        "## Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57k9382BanlI"
      },
      "outputs": [],
      "source": [
        "# < Your Code Here >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYNtoBZ5anlI"
      },
      "source": [
        "## Question & Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6pX5JVlanlI"
      },
      "outputs": [],
      "source": [
        "# < Your Code Here >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP0nHxwUanlI"
      },
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}