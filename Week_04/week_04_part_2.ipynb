{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aglqzo3canlF"
   },
   "source": [
    "---\n",
    "---\n",
    "\n",
    "<h1>Notebook: [ Week #04: Building your own RAG Bot ]</h1>\n",
    "\n",
    "- Your objective in this notebook is create a RAG Bot that allow the users to interact with some notes from AI Champions Bootcamp.\n",
    "- A convenient way to work on this notebook is to open the earlier Jupyter Notebook in `Topic 4`. Yes, the notebook with pre-populated code cells.\n",
    "- You can refer to how a simple RAG Bot (or more like a RAG pipeline) is built\n",
    "- You may extend the functionalities of the bot as you wish.\n",
    "- Minimumly, you should have a simple RAG Bot like the one in the earlier `Topic 4` Jupyter Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyGh4T0wanlG"
   },
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wnQDqT9eanlG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "with open('../openai_key', 'r') as file:\n",
    "    key = file.read().strip()\n",
    "\n",
    "# Set up the OpenAI API key by setting the OPENAI_API_KEY environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIqKLyvhanlG"
   },
   "source": [
    "---\n",
    "\n",
    "## Helper Functions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7EpoVjoanlH"
   },
   "source": [
    "### Function for Generating Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "H4BHqlqjanlH"
   },
   "outputs": [],
   "source": [
    "def get_embedding(input, model='text-embedding-3-small'):\n",
    "    response = client.embeddings.create(\n",
    "        input=input,\n",
    "        model=model\n",
    "    )\n",
    "    return [x.embedding for x in response.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRc55ndeanlH"
   },
   "source": [
    "### Function for Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rozhdpLkanlH"
   },
   "outputs": [],
   "source": [
    "# This is the \"Updated\" helper function for calling LLM\n",
    "def get_completion(prompt, model=\"gpt-4o-mini\", temperature=0, top_p=1.0, max_tokens=256, n=1, json_output=False):\n",
    "    if json_output == True:\n",
    "      output_json_structure = {\"type\": \"json_object\"}\n",
    "    else:\n",
    "      output_json_structure = None\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create( #originally was openai.chat.completions\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=max_tokens,\n",
    "        n=1,\n",
    "        response_format=output_json_structure,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iZ7mi2G0anlH"
   },
   "outputs": [],
   "source": [
    "# This a \"modified\" helper function that we will discuss in this session\n",
    "# Note that this function directly take in \"messages\" as the parameter.\n",
    "def get_completion_by_messages(messages, model=\"gpt-4o-mini\", temperature=0, top_p=1.0, max_tokens=1024, n=1):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=max_tokens,\n",
    "        n=1\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pm9X1jwEanlH"
   },
   "source": [
    "## Functions for Token Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jek79p3oanlH"
   },
   "outputs": [],
   "source": [
    "# These functions are for calculating the tokens.\n",
    "# ⚠️ These are simplified implementations that are good enough for a rough estimation.\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "def count_tokens(text):\n",
    "    encoding = tiktoken.encoding_for_model('gpt-4o-mini')\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def count_tokens_from_message(messages):\n",
    "    encoding = tiktoken.encoding_for_model('gpt-4o-mini')\n",
    "    value = ' '.join([x.get('content') for x in messages])\n",
    "    return len(encoding.encode(value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23QgT89NanlH"
   },
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Create a \"Chat with your Document\" Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukcr0XcHanlH"
   },
   "source": [
    "**\\[ Overview of Steps in RAG \\]**\n",
    "\n",
    "- 1. **Document Loading**\n",
    "\t- In this initial step, relevant documents are ingested and prepared for further processing. This process typically occurs offline.\n",
    "- 2. **Splitting & Chunking**\n",
    "\t- The text from the documents is split into smaller chunks or segments.\n",
    "\t- These chunks serve as the building blocks for subsequent stages.\n",
    "- 3. **Storage**\n",
    "\t- The embeddings (vector representations) of these chunks are created and stored in a vector store.\n",
    "\t- These embeddings capture the semantic meaning of the text.\n",
    "- 4. **Retrieval**\n",
    "\t- When an online query arrives, the system retrieves relevant chunks from the vector store based on the query.\n",
    "\t- This retrieval step ensures that the system identifies the most pertinent information.\n",
    "- 5. **Output**\n",
    "\t- Finally, the retrieved chunks are used to generate a coherent response.\n",
    "\t- This output can be in the form of natural language text, summaries, or other relevant content.\n",
    "\n",
    "![](https://abc-notes.data.tech.gov.sg/resources/img/topic-4-rag-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "So9NhIylanlH"
   },
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdjYmsyLanlH"
   },
   "source": [
    "## Document Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l499JwUWanlI"
   },
   "source": [
    "Here are the \"notes\" that you must include in your RAG pipeline as the `Documents`\n",
    "- [Key Parameters for LLMs](https://abc-notes.data.tech.gov.sg/notes/topic-2-deeper-dive-into-llms/2.-key-parameters-for-llms.html)\n",
    "- [LLMs and Hallucinations](https://abc-notes.data.tech.gov.sg/notes/topic-2-deeper-dive-into-llms/3.-llms-and-hallucinations.html)\n",
    "- [Prompting Techniques for BUilders](https://abc-notes.data.tech.gov.sg/notes/topic-2-deeper-dive-into-llms/4.-prompting-techniques-for-builders.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHhmnnFLanlI"
   },
   "source": [
    "You have three options.\n",
    "1) 💪🏼 Take up the challenge to find a way to get the content directly from the webpages above.\n",
    "2) 🥴 Go with the easy route, download the notes nicely prepared in a `.txt` format. Download the zipped file [here](https://abc-notes.data.tech.gov.sg/resources/data/notes.zip)\n",
    "3) 😎 “Only children choose; adults take all.” Experiment with both data sources and see which can help to the Bot to provide more accurate information for the user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Trmee40xanlI"
   },
   "source": [
    "---\n",
    "\n",
    "> 💡 **Feel free to add as many code cells as your need.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def soup_scrape(url):\n",
    "    \n",
    "    # Send HTTP request\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the page content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "        # Extract all visible text\n",
    "        text = soup.get_text(separator='\\n', strip=True)\n",
    "        return text\n",
    "    else:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MReaOxLZanlI"
   },
   "outputs": [],
   "source": [
    "# < Your Code Here >\n",
    "texts = {\n",
    "    \"key_params\": soup_scrape(\"https://abc-notes.data.tech.gov.sg/notes/topic-2-deeper-dive-into-llms/2.-key-parameters-for-llms.html\"),\n",
    "    \"halu\": soup_scrape(\"https://abc-notes.data.tech.gov.sg/notes/topic-2-deeper-dive-into-llms/3.-llms-and-hallucinations.html\"),\n",
    "    \"prompt_techs\": soup_scrape(\"https://abc-notes.data.tech.gov.sg/notes/topic-2-deeper-dive-into-llms/4.-prompting-techniques-for-builders.html\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lV0Smp3hanlI"
   },
   "source": [
    "## Splitting & Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "def text_split(text):\n",
    "    split_raw = text_splitter.split_text(text)\n",
    "    split_clean = [chunk.replace(\"\\n\", \" \") for chunk in split_raw]\n",
    "    return split_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "v8ZH2ToWanlI"
   },
   "outputs": [],
   "source": [
    "# < Your Code Here >\n",
    "from langchain.schema import Document\n",
    "docs = []\n",
    "for article, text in texts.items():\n",
    "    chunks = text_split(text)\n",
    "    docs.extend([Document(page_content = chunk, metadata = {\"source\": article}) for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'key_params'}, page_content='2. Key Parameters for LLMs icon : LiNotebookTabs'),\n",
       " Document(metadata={'source': 'key_params'}, page_content='Copy Title: Key Parameters for LLMs Tokens'),\n",
       " Document(metadata={'source': 'key_params'}, page_content='Tokens Key Parameters for LLM'),\n",
       " Document(metadata={'source': 'key_params'}, page_content='LLMs and Hallucination'),\n",
       " Document(metadata={'source': 'key_params'}, page_content='Prompting Techniques for Builders')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEEg65AXanlI"
   },
   "source": [
    "## Storage: Embedding & Vectorstores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1O1y1GCdanlI"
   },
   "outputs": [],
   "source": [
    "# < Your Code Here >\n",
    "embeddings_model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    collection_name=\"llm_playbook\",\n",
    "    documents=docs,\n",
    "    embedding=embeddings_model,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not neccesary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3175"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the number of documents in the vector store\n",
    "vector_store._collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['aac2eb8e-ded3-47b5-a47d-50e4665e4b65'],\n",
       " 'embeddings': array([[ 0.01395904, -0.02815107, -0.01866687, ..., -0.02932974,\n",
       "          0.01704963, -0.0009868 ]], shape=(1, 1536)),\n",
       " 'documents': ['2. Key Parameters for LLMs icon : LiNotebookTabs'],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents', 'embeddings'],\n",
       " 'data': None,\n",
       " 'metadatas': [{'source': 'key_params'}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peek at one of the documents in the vector store\n",
    "vector_store._collection.peek(limit=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-dLIQrianlI"
   },
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "57k9382BanlI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='be751e05-2a9b-4860-9979-23e4ecd0deba', metadata={'source': 'prompt_techs'}, page_content='None'),\n",
       " Document(id='2fecc57f-3519-402b-a845-1909b363f4b6', metadata={'source': 'prompt_techs'}, page_content='None'),\n",
       " Document(id='116d30de-aafa-4c27-b3bc-14013dcfabe6', metadata={'source': 'prompt_techs'}, page_content='None')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# < Your Code Here >\n",
    "vector_store.similarity_search('Zero Shot', k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='be751e05-2a9b-4860-9979-23e4ecd0deba', metadata={'source': 'prompt_techs'}, page_content='None'),\n",
       "  0.10985333324070279),\n",
       " (Document(id='2fecc57f-3519-402b-a845-1909b363f4b6', metadata={'source': 'prompt_techs'}, page_content='None'),\n",
       "  0.10985333324070279),\n",
       " (Document(id='116d30de-aafa-4c27-b3bc-14013dcfabe6', metadata={'source': 'prompt_techs'}, page_content='None'),\n",
       "  0.10985333324070279)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search_with_relevance_scores('Zero Shot', k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYNtoBZ5anlI"
   },
   "source": [
    "## Question & Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "N6pX5JVlanlI"
   },
   "outputs": [],
   "source": [
    "# < Your Code Here >\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    ChatOpenAI(model='gpt-4o-mini'),\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    return_source_documents=True, # Make inspection of document possible\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "OP0nHxwUanlI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Why LLM hallucinate?',\n",
       " 'result': 'LLMs may hallucinate due to their reliance on statistical patterns and associations in training data, which can lead to generating plausible but false information. Additionally, they sometimes lack true understanding of context, resulting in incorrect or nonsensical outputs. Thanks for asking!',\n",
       " 'source_documents': [Document(id='d8648a7c-5c48-495f-a682-8636e42f5e3c', metadata={'source': 'key_params'}, page_content='LLMs and Hallucination'),\n",
       "  Document(id='d85cb3cc-b79c-43a9-bcc8-2590b68e9d1c', metadata={'source': 'prompt_techs'}, page_content='LLMs and Hallucination'),\n",
       "  Document(id='ab98b484-d1e8-4043-aa7e-4501f281ed49', metadata={'source': 'halu'}, page_content='LLMs and Hallucination'),\n",
       "  Document(id='0af4f85d-2af4-4927-bc79-714269cafd4e', metadata={'source': 'key_params'}, page_content='LLMs and Hallucination')]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.invoke(\"Why LLM hallucinate?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
