{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6203758e-73eb-4e7b-ad76-d41d604345c6",
      "metadata": {
        "id": "6203758e-73eb-4e7b-ad76-d41d604345c6"
      },
      "source": [
        "---\n",
        "---\n",
        "# Notebook: [ Week #05: RAG Advanced Retrieval and Evaluation ]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o8rNnmfpKgf0"
      },
      "id": "o8rNnmfpKgf0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "496cd6c8-1d46-4f5c-9cf6-d6369cc52c1b",
      "metadata": {
        "id": "496cd6c8-1d46-4f5c-9cf6-d6369cc52c1b"
      },
      "source": [
        "## Setup\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8822c1c-6212-416a-aa2a-58d2bf2560c0",
      "metadata": {
        "id": "d8822c1c-6212-416a-aa2a-58d2bf2560c0"
      },
      "outputs": [],
      "source": [
        "!pip install openai --quiet\n",
        "!pip install langchain --quiet\n",
        "!pip install langchain_openai --quiet\n",
        "!pip install langchain-community --quiet\n",
        "!pip install unstructured --quiet\n",
        "!pip install pdfminer --quiet\n",
        "!pip install langchain-experimental --quiet\n",
        "!pip install langchain_cohere --quiet\n",
        "!pip install pysbd --quiet\n",
        "!pip install ragas --quiet\n",
        "!pip install pypdf --quiet\n",
        "!pip install lolviz --quiet\n",
        "!pip install chromadb --quiet\n",
        "!pip install pdfminer.six --quiet\n",
        "!pip install tiktoken --quiet\n",
        "!pip install datasets --quiet\n",
        "!pip install langchain-chroma --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37b18427-b945-4641-a0b7-7ca5427226c0",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-28T12:00:24.632121Z",
          "start_time": "2024-04-28T12:00:20.490138Z"
        },
        "id": "37b18427-b945-4641-a0b7-7ca5427226c0",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from getpass import getpass\n",
        "\n",
        "API_KEY = getpass(\"Enter your OpenAI API Key\")\n",
        "client = OpenAI(api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4d07baa",
      "metadata": {
        "id": "c4d07baa"
      },
      "source": [
        "---\n",
        "\n",
        "## Helper Functions\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b03eda8",
      "metadata": {
        "id": "2b03eda8"
      },
      "source": [
        "### Function for Generating Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32f1fb11",
      "metadata": {
        "id": "32f1fb11"
      },
      "outputs": [],
      "source": [
        "def get_embedding(input, model='text-embedding-3-small'):\n",
        "    response = client.embeddings.create(\n",
        "        input=input,\n",
        "        model=model\n",
        "    )\n",
        "    return [x.embedding for x in response.data]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3435d399",
      "metadata": {
        "id": "3435d399"
      },
      "source": [
        "### Function for Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce3e807b",
      "metadata": {
        "id": "ce3e807b"
      },
      "outputs": [],
      "source": [
        "# This is the \"Updated\" helper function for calling LLM\n",
        "def get_completion(prompt, model=\"gpt-4o-mini\", temperature=0, top_p=1.0, max_tokens=256, n=1, json_output=False):\n",
        "    if json_output == True:\n",
        "      output_json_structure = {\"type\": \"json_object\"}\n",
        "    else:\n",
        "      output_json_structure = None\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = client.chat.completions.create( #originally was openai.chat.completions\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        max_tokens=max_tokens,\n",
        "        n=1,\n",
        "        response_format=output_json_structure,\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "614c7877",
      "metadata": {
        "id": "614c7877"
      },
      "outputs": [],
      "source": [
        "# This a \"modified\" helper function that we will discuss in this session\n",
        "# Note that this function directly take in \"messages\" as the parameter.\n",
        "def get_completion_by_messages(messages, model=\"gpt-4o-mini\", temperature=0, top_p=1.0, max_tokens=1024, n=1):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        max_tokens=max_tokens,\n",
        "        n=1\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a503547-3c2e-4b63-82a4-33e6da38e7a4",
      "metadata": {
        "id": "2a503547-3c2e-4b63-82a4-33e6da38e7a4"
      },
      "source": [
        "### Functions for Token Counting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7210eb23-50f8-4681-84a2-493f4708501a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-28T12:00:27.880949Z",
          "start_time": "2024-04-28T12:00:27.870060Z"
        },
        "id": "7210eb23-50f8-4681-84a2-493f4708501a"
      },
      "outputs": [],
      "source": [
        "# This function is for calculating the tokens given the \"message\"\n",
        "# ⚠️ This is simplified implementation that is good enough for a rough estimation\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "def count_tokens(text):\n",
        "    encoding = tiktoken.encoding_for_model('gpt-4o-mini')\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "def count_tokens_from_message_rough(messages):\n",
        "    encoding = tiktoken.encoding_for_model('gpt-4o-mini')\n",
        "    value = ' '.join([x.get('content') for x in messages])\n",
        "    return len(encoding.encode(value))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43375ba7e5dba945",
      "metadata": {
        "id": "43375ba7e5dba945"
      },
      "source": [
        "## Setting up Credentials & Common Components for LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2039d84463a85a43",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-28T12:00:28.844113Z",
          "start_time": "2024-04-28T12:00:28.841045Z"
        },
        "id": "2039d84463a85a43"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "os.environ[\"OPENAI_API_KEY\"] = API_KEY\n",
        "\n",
        "# embedding model that we will use for the session\n",
        "embeddings_model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
        "\n",
        "# llm to be used in RAG pipeplines in this notebook\n",
        "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23ea3f59",
      "metadata": {
        "id": "23ea3f59"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1786f104c7a6e1a",
      "metadata": {
        "id": "f1786f104c7a6e1a"
      },
      "source": [
        "# Setting up a Naive RAG\n",
        "\n",
        "![](https://d27l3jncscxhbx.cloudfront.net/lib/media/img-20240421132947558.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d11f82d1",
      "metadata": {
        "id": "d11f82d1"
      },
      "source": [
        "**\\[ Overview of Steps in RAG \\]**\n",
        "\n",
        "- 1. **Document Loading**\n",
        "\t- In this initial step, relevant documents are ingested and prepared for further processing. This process typically occurs offline.\n",
        "- 2. **Splitting & Chunking**\n",
        "\t- The text from the documents is split into smaller chunks or segments.\n",
        "\t- These chunks serve as the building blocks for subsequent stages.\n",
        "- 3. **Storage**\n",
        "\t- The embeddings (vector representations) of these chunks are created and stored in a vector store.\n",
        "\t- These embeddings capture the semantic meaning of the text.\n",
        "- 4. **Retrieval**\n",
        "\t- When an online query arrives, the system retrieves relevant chunks from the vector store based on the query.\n",
        "\t- This retrieval step ensures that the system identifies the most pertinent information.\n",
        "- 5. **Output**\n",
        "\t- Finally, the retrieved chunks are used to generate a coherent response.\n",
        "\t- This output can be in the form of natural language text, summaries, or other relevant content.\n",
        "\n",
        "![](https://abc-notes.data.tech.gov.sg/resources/img/topic-4-rag-overview.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "509e2a85",
      "metadata": {
        "id": "509e2a85"
      },
      "source": [
        "## Importing Dependencies and Setting up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb607c36e1f155e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-28T12:00:31.142961Z",
          "start_time": "2024-04-28T12:00:30.800805Z"
        },
        "id": "cb607c36e1f155e"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f172c36",
      "metadata": {
        "id": "2f172c36"
      },
      "outputs": [],
      "source": [
        "# Download and unzip into local folder\n",
        "url = \"https://abc-notes.data.tech.gov.sg/resources/data/notes.zip\"\n",
        "\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "\n",
        "response = requests.get(url)\n",
        "z = zipfile.ZipFile(io.BytesIO(response.content))\n",
        "z.extractall()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27c59df1",
      "metadata": {
        "id": "27c59df1"
      },
      "source": [
        "## Document Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ef5e533",
      "metadata": {
        "id": "3ef5e533"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e35df19",
      "metadata": {
        "id": "6e35df19"
      },
      "outputs": [],
      "source": [
        "# list of filenames to load\n",
        "filename_list = [\n",
        "    '2. Key Parameters for LLMs.txt',\n",
        "    '3. LLMs and Hallucinations.txt',\n",
        "    '4. Prompting Techniques for Builders.txt',\n",
        "]\n",
        "\n",
        "# load the documents\n",
        "list_of_documents_loaded = []\n",
        "for filename in filename_list:\n",
        "    try:\n",
        "        # try to load the document\n",
        "        markdown_path = os.path.join('notes', filename)\n",
        "        loader = TextLoader(markdown_path)\n",
        "\n",
        "        # load() returns a list of Document objects\n",
        "        data = loader.load()\n",
        "        # use extend() to add to the list_of_documents_loaded\n",
        "        list_of_documents_loaded.extend(data)\n",
        "        print(f\"Loaded {filename}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # if there is an error loading the document, print the error and continue to the next document\n",
        "        print(f\"Error loading {filename}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(\"Total documents loaded:\", len(list_of_documents_loaded))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8baa889",
      "metadata": {
        "id": "f8baa889"
      },
      "source": [
        "## Splitting and Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb5f49ec",
      "metadata": {
        "id": "cb5f49ec"
      },
      "outputs": [],
      "source": [
        "# See the first document loaded\n",
        "list_of_documents_loaded[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_documents_loaded[0].metadata.get(\"source\")"
      ],
      "metadata": {
        "id": "ysfcd2jnPnrD"
      },
      "id": "ysfcd2jnPnrD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "303d8cac",
      "metadata": {
        "id": "303d8cac"
      },
      "outputs": [],
      "source": [
        "# With the understanding of the structure, we can now extract the page_content\n",
        "# Check the number of tokens per document\n",
        "i = 0\n",
        "for doc in list_of_documents_loaded:\n",
        "    i += 1\n",
        "    print(f'Document {i} - \"{doc.metadata.get(\"source\")}\" has {count_tokens(doc.page_content)} tokens')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33d5b029",
      "metadata": {
        "id": "33d5b029"
      },
      "outputs": [],
      "source": [
        "# While our document is not too long, we can still split it into smaller chunks\n",
        "# This is to ensure that we can process the document in smaller chunks\n",
        "# This is especially useful for long documents that may exceed the token limit\n",
        "# or to keep the chunks smaller, so each chunk is more focused\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# In this case, we intentionally set the chunk_size to 1100 tokens, to have the smallest document (document 2) intact\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1100, chunk_overlap=10, length_function=count_tokens)\n",
        "\n",
        "# Split the documents into smaller chunks\n",
        "splitted_documents = text_splitter.split_documents(list_of_documents_loaded)\n",
        "\n",
        "# Print the number of documents after splitting\n",
        "print(f\"Number of documents after splitting: {len(splitted_documents)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cb93f263486d65",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-28T12:00:47.650861Z",
          "start_time": "2024-04-28T12:00:42.350151Z"
        },
        "id": "3cb93f263486d65"
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "# Create the vector database\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=splitted_documents,\n",
        "    embedding=embeddings_model,\n",
        "    collection_name=\"naive_splitter\", # one database can have multiple collections\n",
        "    persist_directory=\"./vector_db\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "044b4030",
      "metadata": {
        "id": "044b4030"
      },
      "outputs": [],
      "source": [
        "# Create the RAG pipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# The `llm` is defined earlier in the notebook (using GPT-4o-mini)\n",
        "rag_chain = RetrievalQA.from_llm(\n",
        "    retriever=vectordb.as_retriever(), llm=llm\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce887436",
      "metadata": {
        "id": "ce887436"
      },
      "outputs": [],
      "source": [
        "# Now we can use the RAG pipeline to ask questions\n",
        "# Let's ask a question that we know is in the documents\n",
        "llm_response = rag_chain.invoke('What is Top-P sampling?')\n",
        "print(llm_response['result'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d0640d2",
      "metadata": {
        "id": "4d0640d2"
      },
      "outputs": [],
      "source": [
        "# Now let's break down and see what are the \"splitted_documents\" that are used in the RAG pipeline\n",
        "# We can do this by using the vectordb object that we have created\n",
        "# k=4 is the default value for the number of retrieved documents\n",
        "retrieved_documents = vectordb.similarity_search_with_relevance_scores(\"What is Top-P sampling?\", k=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83e1da1c",
      "metadata": {
        "id": "83e1da1c"
      },
      "outputs": [],
      "source": [
        "retrieved_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "066691c0",
      "metadata": {
        "id": "066691c0"
      },
      "outputs": [],
      "source": [
        "# Display the first retrieved documents\n",
        "# Feel free to change the index to see other retrieved documents\n",
        "# Note that each of the documents is a tuple of (Document, relevance_score)\n",
        "# the second [0] in [0][0] below is to get the Document object\n",
        "print(retrieved_documents[0][0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7253474f",
      "metadata": {
        "id": "7253474f"
      },
      "outputs": [],
      "source": [
        "# Now we can use the RAG pipeline to ask questions\n",
        "# Let's ask a question that we know is NOT in the documents\n",
        "llm_response = rag_chain.invoke('What is an LLM Agent?')\n",
        "print(llm_response['result'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae875743",
      "metadata": {
        "id": "ae875743"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f584e434",
      "metadata": {
        "id": "f584e434"
      },
      "outputs": [],
      "source": [
        "# Compared to the rag pipelines that we used above, this cell allows a custom prompt to be used\n",
        "# This is useful for customizing the prompt to be used in the retrieval QA chain\n",
        "# The prompt below is the standard template that is used in the retrieval QA chain\n",
        "# It also includes the \"documents\" that are used in the prompt\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Build prompt\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
        "\n",
        "# Run chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    ChatOpenAI(model='gpt-4o-mini'),\n",
        "    retriever=vectordb.as_retriever(),\n",
        "    return_source_documents=True, # Make inspection of document possible\n",
        "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa3c9a9a",
      "metadata": {
        "id": "aa3c9a9a"
      },
      "outputs": [],
      "source": [
        "qa_chain.invoke('What is Top-P sampling?')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaafe40a90410ca",
      "metadata": {
        "id": "aaafe40a90410ca"
      },
      "source": [
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55b935ac",
      "metadata": {
        "id": "55b935ac"
      },
      "source": [
        "# Techniques for Improving RAG Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "017a6b5f",
      "metadata": {
        "id": "017a6b5f"
      },
      "source": [
        "RAG is only as good as the retrieved documents’ relevance and quality. Fortunately, an emerging set of techniques can be employed to design and improve RAG systems.\n",
        "\n",
        "- The improvement of RAG is not just a matter of incremental updates, by installing newer Python package or calling any functions out-of-the-box, but many of them involves a comprehensive rethinking of its architecture and processes.\n",
        "\n",
        "- We can group the various improvements under 3 major categories:\n",
        "    - ✦ **Pre-Retrieval Processes**\n",
        "    - ✦ **Retrieval Process**\n",
        "    - ✦ **Post-Retrieval Process**\n",
        "\n",
        "![](https://abc-notes.data.tech.gov.sg/notes/_resources/topic-5-advanced-rag/img-20240427230330556.png)\n",
        "\n",
        "<small>Note: This categorization is intended for organizational purposes and may not be entirely precise. It's important to recognize that some techniques can impact multiple stages of the RAG process.</small>\n",
        "\n",
        "---\n",
        "\n",
        "- **How to use these techniques**?\n",
        "    - Think of these techniques like Lego blocks – each one offers a unique function to improve the overall RAG system. The beauty lies in the experimentation, in building different combinations of these blocks to find the optimal configuration for a specific task.\n",
        "\n",
        "    - There's no one-size-fits-all solution, no silver bullet recipe for success. The accuracy of a RAG system depends on a multitude of factors, including the structure of the documents, the nature of the questions, and even the specific dataset being used.\n",
        "\n",
        "    - A combination that works wonders for one case might fall flat in another. The key is to embrace the modularity of these techniques and explore different configurations to find the perfect fit for your specific needs.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e428f081",
      "metadata": {
        "id": "e428f081"
      },
      "source": [
        "---\n",
        "---\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43a179a7-c5e5-4d7d-961d-1aa4749ba54b",
      "metadata": {
        "id": "43a179a7-c5e5-4d7d-961d-1aa4749ba54b"
      },
      "source": [
        "# Improving the Pre-Retrieval Process\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af3e6276e962cd72",
      "metadata": {
        "id": "af3e6276e962cd72"
      },
      "source": [
        "## Semantic Chunking\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a87f6885a8ac649a",
      "metadata": {
        "id": "a87f6885a8ac649a"
      },
      "source": [
        "- Semantic chunking relies heavily on embeddings, powerful tools for understanding text semantics.\n",
        "\n",
        "- Despite their apparent simplicity, embeddings enable the comparison of different text segments and the identification of underlying relationships.\n",
        "\n",
        "- Here’s how semantic chunking works in practice:\n",
        "    - Text segments with similar meanings are grouped together.\n",
        "    - Leveraging embeddings, we analyze and group consecutive sentences within a specified window size.\n",
        "    - Beginning with the initial sentence, we compare its embedding to the subsequent sentences, iterating through the text until a significant deviation is detected, indicating a potential break point.\n",
        "    - Continuously computing embeddings within each sentence set allows for dynamic adjustments, refining the grouping process and enhancing our understanding of the text’s meaning.\n",
        "    - Through this method, we identify coherent groups of sentences that form meaningful sections, aiding in analysis and comprehension.\n",
        "- The easiest way to take advantage of this cutting-edge chunking approach is to use Langchain's experimental module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82gHJ8fHQrBX",
      "metadata": {
        "id": "82gHJ8fHQrBX"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "# Create the text splitter\n",
        "text_splitter = SemanticChunker(embeddings_model)\n",
        "\n",
        "# Split the documents into smaller chunks\n",
        "splitted_documents = text_splitter.split_documents(list_of_documents_loaded)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KUs8y4zXRgmV",
      "metadata": {
        "id": "KUs8y4zXRgmV"
      },
      "outputs": [],
      "source": [
        "# View the first splitted document\n",
        "# Feel free to change the index to see other splitted documents\n",
        "# Check where the split is done\n",
        "import pprint\n",
        "\n",
        "pprint.pprint(dict(splitted_documents[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc80f310",
      "metadata": {
        "id": "fc80f310"
      },
      "outputs": [],
      "source": [
        "# Apparently, the concept \"Temperature\" has been split into two different chunks\n",
        "import pprint\n",
        "\n",
        "pprint.pprint(dict(splitted_documents[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6c0cdce5eb4ba33",
      "metadata": {
        "id": "a6c0cdce5eb4ba33"
      },
      "outputs": [],
      "source": [
        "# Embed each chunk and load it into the vector store.\n",
        "vectordb = Chroma.from_documents(splitted_documents, embeddings_model, collection_name='embedding_semantic', persist_directory='./vector_db')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c81e29bcf18c3049",
      "metadata": {
        "id": "c81e29bcf18c3049"
      },
      "outputs": [],
      "source": [
        "# Create a Q&A Chain (Pipeline) using LangChain\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "\n",
        "retriever_chain_from_llm = RetrievalQA.from_llm(\n",
        "    retriever=vectordb.as_retriever(), llm=llm\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d33d717a60a0eac6",
      "metadata": {
        "id": "d33d717a60a0eac6"
      },
      "outputs": [],
      "source": [
        "# Let's see if the LLM can answer the question \"What is Temperature in LLMs?\" well\n",
        "retriever_chain_from_llm.invoke('What is Temperature in LLMs?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J8VpRxkXS4e2",
      "metadata": {
        "id": "J8VpRxkXS4e2"
      },
      "outputs": [],
      "source": [
        "# In this result, you can see the second chunk of the \"Temperature\" concept is retrieved but at the last position\n",
        "# This is still fine since all the chunks are retrieved and passed to the LLM for answering the question\n",
        "vectordb.similarity_search_with_relevance_scores('What is Temperature in LLMs?', k=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ls8pSxwfNpjt",
      "metadata": {
        "id": "Ls8pSxwfNpjt"
      },
      "source": [
        "> 💡 Try with different questions for the cell above."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PPp288c7Ntv2",
      "metadata": {
        "id": "PPp288c7Ntv2"
      },
      "source": [
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44d6535681160319",
      "metadata": {
        "id": "44d6535681160319"
      },
      "source": [
        "## Multi Query Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73efc6f860c8b6a2",
      "metadata": {
        "id": "73efc6f860c8b6a2"
      },
      "source": [
        "- If query is complex and having multiple context then, retrieval with the single query may not be the good approach as it may fail to get proper output.\n",
        "\n",
        "- In LangChain, the `MultiQueryRetriever` automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query.\n",
        "\n",
        "- For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents.\n",
        "\n",
        "- By generating multiple perspectives on the same question, the `MultiQueryRetriever` might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results.\n",
        "\n",
        "- Below is a sample implementation using MultiQueryRetriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa7b9a1f7f17e6c2",
      "metadata": {
        "id": "aa7b9a1f7f17e6c2"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "# We will be reusing the `vectordb` from the Naive RAG\n",
        "# You can imagine `MultiQueryRetriever` as a chain that generates multiple queries\n",
        "# itself is not a complete RAG chain, but it can be used as a retriever in a RAG chain\n",
        "retriever_multiquery = MultiQueryRetriever.from_llm(\n",
        "  retriever=vectordb.as_retriever(), llm=llm,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0fad38eacc0fb69",
      "metadata": {
        "id": "f0fad38eacc0fb69"
      },
      "source": [
        "---\n",
        "\n",
        "Let's peek into the `MultiQueryRetriever` class to understand how it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "690d647f1af7f2bb",
      "metadata": {
        "id": "690d647f1af7f2bb"
      },
      "outputs": [],
      "source": [
        "# Set logging for the queries\n",
        "import logging\n",
        "\n",
        "# Refer to LangChain documentation to find which loggers to set\n",
        "# Different LangChain Classes/Modules have different loggers to set\n",
        "logging.basicConfig()\n",
        "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "347f659a7742bd3b",
      "metadata": {
        "id": "347f659a7742bd3b"
      },
      "outputs": [],
      "source": [
        "question = \"What is Temperature in LLMs?\"\n",
        "\n",
        "unique_docs = retriever_multiquery.invoke(question)\n",
        "len(unique_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0H5MN67tDABW",
      "metadata": {
        "id": "0H5MN67tDABW"
      },
      "outputs": [],
      "source": [
        "# Create the compplete Q&A Chain (Pipeline)\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Note that the `retriever_multiquery` is used as the retriever\n",
        "# rather than the `vectordb` and its as_retriever() method that we used in the previous examples\n",
        "qa_chain_multiquery= RetrievalQA.from_llm(\n",
        "    retriever=retriever_multiquery, llm=llm\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CAYZ3gjvDNrS",
      "metadata": {
        "id": "CAYZ3gjvDNrS"
      },
      "outputs": [],
      "source": [
        "# The first output lists the various queries that are generated\n",
        "qa_chain_multiquery.invoke(\"What is temperature in LLMs?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc066c49",
      "metadata": {
        "id": "bc066c49"
      },
      "source": [
        "<br>\n",
        "\n",
        "### 🔷 Challenge 1\n",
        "- Use OpenAI LLM to improve the user query first, before passing the LLM-improved query into the `qa_chain_multiquery` above.\n",
        "- What will be the `prompt` that you will use?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "665e686d",
      "metadata": {
        "id": "665e686d"
      },
      "outputs": [],
      "source": [
        "# < Your Code Here >"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58c989241c46341f",
      "metadata": {
        "id": "58c989241c46341f"
      },
      "source": [
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb80129a421cfbdf",
      "metadata": {
        "id": "fb80129a421cfbdf"
      },
      "source": [
        "# Improving the Retrieval Process\n",
        "\n",
        "![](https://abc-notes.data.tech.gov.sg/notes/_resources/topic-5-advanced-rag/img-20240427225235075.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02de73e8",
      "metadata": {
        "id": "02de73e8"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f25d259d",
      "metadata": {
        "id": "f25d259d"
      },
      "source": [
        "## Maximum Marginal Relevance (MMR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8c96337",
      "metadata": {
        "id": "f8c96337"
      },
      "source": [
        "In the domain of information retrieval and text summarization, the challenge often lies in finding a balance between relevance and diversity. This is where Maximum Marginal Relevance (MMR) comes into play.\n",
        "- MMR is based on the idea that while retrieving information or summarizing text, one should not only focus on the relevance of the content but also on how different the selected items are from each other.\n",
        "- This helps in reducing redundancy and improving the overall quality of the retrieved information or summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4935e713",
      "metadata": {
        "id": "4935e713"
      },
      "outputs": [],
      "source": [
        "# In this exampe, we will take a pause from the previous documents (notes from AI Champions Bootcamp).\n",
        "# We will work with a new set of texts, which able to better demonstrate the issues that MMR can solve\n",
        "\n",
        "texts = [\n",
        "    \"\"\"The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).\"\"\",\n",
        "    \"\"\"A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.\"\"\",\n",
        "    \"\"\"A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.\"\"\",\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60e8f5f6",
      "metadata": {
        "id": "60e8f5f6"
      },
      "source": [
        "### Ordinary Similarity-based Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11663d75",
      "metadata": {
        "id": "11663d75"
      },
      "outputs": [],
      "source": [
        "# Embed each chunk and load it into the vector store.\n",
        "vectordb = Chroma.from_texts(texts, embeddings_model, collection_name='mushroom')\n",
        "\n",
        "print(vectordb._collection.count())\n",
        "\n",
        "\n",
        "# Let's see the underlying documents that are used in the retrieval QA chain\n",
        "# This time, instead of using vectordb.similarity_search_with_relevance_scores\n",
        "# we use the `retriever` object that is created from the `vectordb`\n",
        "# `retriever` is a high-level object that is used in the retrieval QA chain\n",
        "# It supports more features and allow us to modify the retrieval process,\n",
        "# including MMR, set the similarity threshold, and more\n",
        "\n",
        "retriever = vectordb.as_retriever(search_kwargs={'k': 2})\n",
        "\n",
        "retriever_documents = retriever.invoke('Tell me about all-white mushrooms with large fruiting bodies')\n",
        "\n",
        "# Note that while the documents are retrieved to maximize the relevance,\n",
        "# the 3rd document is not retrieved as it is not relevant to the query,\n",
        "# but yet it contains a critical piecce of information to be included in the answer\n",
        "retriever_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e1c76ce",
      "metadata": {
        "id": "2e1c76ce"
      },
      "outputs": [],
      "source": [
        "# Create the compplete Q&A Chain (Pipeline)\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain_mushroom = RetrievalQA.from_llm(\n",
        "    retriever=retriever, llm=llm\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d539c5a",
      "metadata": {
        "id": "0d539c5a"
      },
      "outputs": [],
      "source": [
        "question = \"Tell me about white mushrooms with large fruiting bodies.\"\n",
        "\n",
        "# Note that fact that the mushroom might be poisonous is not included in the retrieved documents\n",
        "# and are not included in the answer\n",
        "qa_chain_mushroom.invoke(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e588dff1",
      "metadata": {
        "id": "e588dff1"
      },
      "source": [
        "### MMR-based Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7404876",
      "metadata": {
        "id": "d7404876"
      },
      "outputs": [],
      "source": [
        "# Note that we do not have to change the vector database to use MMR\n",
        "# We can use the `retriever` object to set the MMR parameters\n",
        "# The MMR is a built-in feature in the `retriever` object for some vector stores\n",
        "\n",
        "# Remember that the `retriever` object is a high-level object that is used in the retrieval QA chain\n",
        "retriever = vectordb.as_retriever(search_type='mmr',\n",
        "                                  search_kwargs={'k': 2, 'fetch_k': 3})\n",
        "\n",
        "retriever_documents = retriever.invoke('Tell me about all-white mushrooms with large fruiting bodies')\n",
        "\n",
        "# Note that due to the MMR, the 3rd document is now retrieved,\n",
        "# as MMR introduces diversity in the retrieved documents.\n",
        "# Of course in actual cases, we will use larger `fetch_k`\n",
        "# and reasonable 'k' values to get more diverse documents and sufficient context\n",
        "retriever_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b66b73e",
      "metadata": {
        "id": "8b66b73e"
      },
      "outputs": [],
      "source": [
        "question = \"Tell me about white mushrooms with large fruiting bodies.\"\n",
        "\n",
        "# Note that now the response includes the fact that the mushroom is poisonous\n",
        "\n",
        "qa_chain_mushroom = RetrievalQA.from_llm(\n",
        "    retriever=retriever, llm=llm\n",
        ")\n",
        "qa_chain_mushroom.invoke(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "975fb997",
      "metadata": {
        "id": "975fb997"
      },
      "source": [
        "> 💡**OPTIONAL: If you're keen to find out more on how to use `as_retriever()` method, click the link below:\n",
        "\n",
        "<details>\n",
        "<summary><font size=\"2\" color=\"darkgreen\"><b>👆🏼 Click to for find out)</b></font></summary>\n",
        "\n",
        "<small>\n",
        "The term np.max(logits) is subtracted from logits to avoid numerical instability that can occur when taking the exponential of large numbers, a common issue in machine learning computations.\n",
        "\n",
        "Here's why: the softmax function involves taking the exponential of each logit. If a logit is a large positive number, its exponential can be extremely large - so large that it exceeds the maximum representable number (overflow), leading to inf values. This can cause the softmax function to return incorrect results.\n",
        "\n",
        "By subtracting np.max(logits), we ensure that the maximum value in the logits array is 0, which means the largest possible output from the exponential function is 1. This effectively eliminates the possibility of overflow.\n",
        "\n",
        "Importantly, this operation doesn't change the output of the softmax function. That's because softmax is shift invariant, meaning that adding or subtracting a constant from each logit doesn't affect the output probabilities. This property allows us to subtract the maximum logit for numerical stability without changing the function's output.\n",
        "\n",
        "\n",
        "### **Documentation for `as_retriever()` method**\n",
        "\n",
        "- Signature: vectordb.as_retriever(**kwargs: 'Any') -> 'VectorStoreRetriever'\n",
        "\n",
        "- Docstring:\n",
        "    - Return VectorStoreRetriever initialized from this VectorStore.\n",
        "\n",
        "- Args:\n",
        "    - **kwargs: Keyword arguments to pass to the search function.\n",
        "        - Can include:\n",
        "        - `search_type` (Optional[str]): Defines the type of search that\n",
        "            the Retriever should perform.\n",
        "            Can be \"similarity\" (default), \"mmr\", or\n",
        "            \"similarity_score_threshold\".\n",
        "        - `search_kwargs` (Optional[Dict]): Keyword arguments to pass to the\n",
        "            search function. Can include things like:\n",
        "            - `k`: Amount of documents to return (Default: 4)\n",
        "            - `score_threshold`: Minimum relevance threshold for similarity_score_threshold\n",
        "            - `fetch_k`: Amount of documents to pass to MMR algorithm (Default: 20)\n",
        "            - `lambda_mult`: Diversity of results returned by MMR; 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
        "            - `filter`: Filter by document metadata\n",
        "\n",
        "- Returns:\n",
        "    VectorStoreRetriever: Retriever class for VectorStore.\n",
        "\n",
        "- Examples:\n",
        "\n",
        "```Python\n",
        "\n",
        "    # Retrieve more documents with higher diversity\n",
        "    # Useful if your dataset has many similar documents\n",
        "    docsearch.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs={'k': 6, 'lambda_mult': 0.25}\n",
        "    )\n",
        "\n",
        "    # Fetch more documents for the MMR algorithm to consider\n",
        "    # But only return the top 5\n",
        "    docsearch.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs={'k': 5, 'fetch_k': 50}\n",
        "    )\n",
        "\n",
        "    # Only retrieve documents that have a relevance score\n",
        "    # Above a certain threshold\n",
        "    docsearch.as_retriever(\n",
        "        search_type=\"similarity_score_threshold\",\n",
        "        search_kwargs={'score_threshold': 0.8}\n",
        "    )\n",
        "\n",
        "    # Only get the single most similar document from the dataset\n",
        "    docsearch.as_retriever(search_kwargs={'k': 1})\n",
        "\n",
        "    # Use a filter to only retrieve documents from a specific paper\n",
        "    docsearch.as_retriever(\n",
        "        search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}}\n",
        "    )\n",
        "```\n",
        "</small>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2457e4b-74e3-486a-bc7c-f09089cd227a",
      "metadata": {
        "id": "d2457e4b-74e3-486a-bc7c-f09089cd227a"
      },
      "source": [
        "<br>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a661ce7-9bb5-4d16-8bf9-c9fe131d2b37",
      "metadata": {
        "id": "8a661ce7-9bb5-4d16-8bf9-c9fe131d2b37"
      },
      "source": [
        "## Parent Child Index Retrieval\n",
        "\n",
        "When splitting documents for retrieval, there are often conflicting desires:\n",
        "\n",
        "You may want to have small documents, so that their embeddings can most accurately reflect their meaning. If too long, then the embeddings can lose meaning.\n",
        "You want to have long enough documents that the context of each chunk is retained.\n",
        "The ParentDocumentRetriever strikes that balance by splitting and storing small chunks of data. During retrieval, it first fetches the small chunks but then looks up the parent ids for those chunks and returns those larger documents.\n",
        "\n",
        "Note that \"parent document\" refers to the document that a small chunk originated from. This can either be the whole raw document OR a larger chunk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa24b9a5",
      "metadata": {
        "id": "fa24b9a5"
      },
      "outputs": [],
      "source": [
        "# Since we are trying out different examples in this notebook\n",
        "# This is to reset the collection in the vector store to have a clean slate\n",
        "vectordb.reset_collection()\n",
        "vectordb._collection.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28a3475a-488d-4fb8-b76d-222ef0c0add6",
      "metadata": {
        "id": "28a3475a-488d-4fb8-b76d-222ef0c0add6"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fefe5ea2074752c9",
      "metadata": {
        "id": "fefe5ea2074752c9"
      },
      "outputs": [],
      "source": [
        "# This text splitter is used to create the parent documents\n",
        "parent_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n# \"], chunk_size=4000, length_function=count_tokens)\n",
        "\n",
        "# This text splitter is used to create the child documents\n",
        "# It should create documents smaller than the parent\n",
        "child_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n## \"], chunk_size=1250, length_function=count_tokens)\n",
        "\n",
        "# The vectorstore to use to index the child chunks\n",
        "vectordb = Chroma(collection_name=\"parent_child\", embedding_function=embeddings_model)\n",
        "\n",
        "# The storage layer for the parent documents\n",
        "store = InMemoryStore()\n",
        "\n",
        "# Specificy a Retriever\n",
        "retriever = ParentDocumentRetriever(\n",
        "    vectorstore=vectordb,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        "    parent_splitter=parent_splitter,\n",
        "    search_kwargs={'k': 4}\n",
        ")\n",
        "\n",
        "# The splitting & embeddings happen\n",
        "retriever.add_documents(list_of_documents_loaded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81346dd9",
      "metadata": {
        "id": "81346dd9"
      },
      "outputs": [],
      "source": [
        "# The documents are indexed\n",
        "# 4 parents because the last document is split into 2 parts\n",
        "store.store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1e7165a",
      "metadata": {
        "id": "a1e7165a"
      },
      "outputs": [],
      "source": [
        "# Get all records in vector store\n",
        "vectordb._collection.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e27c299f",
      "metadata": {
        "id": "e27c299f"
      },
      "outputs": [],
      "source": [
        "# Rely on the vectorstore to retrieve all the documents (which we are familiar with)\n",
        "# That that there are 9 child documents\n",
        "vectordb.similarity_search_with_relevance_scores(\"What is Top-P sampling?\", k=15)\n",
        "\n",
        "# Alternatively, we can use\n",
        "# vectordb._collection.peek()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fd614cb7a5793f7",
      "metadata": {
        "id": "9fd614cb7a5793f7"
      },
      "outputs": [],
      "source": [
        "# We use the underlying vector store to retrieves the small chunks.\n",
        "sub_docs = vectordb.similarity_search('What is Top-P sampling?')\n",
        "sub_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f47b45e1",
      "metadata": {
        "id": "f47b45e1"
      },
      "outputs": [],
      "source": [
        "# This is the parent-level retriever that returns the parent documents (the larger chunks)\n",
        "retrieved_docs = retriever.invoke('What is Top-P sampling?')\n",
        "retrieved_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24515733",
      "metadata": {
        "id": "24515733"
      },
      "source": [
        "> For this technique, we will skip the complete RAG Pipeline creation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b6b5e7b",
      "metadata": {
        "id": "3b6b5e7b"
      },
      "source": [
        "---\n",
        "---\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51e79f75401a38ab",
      "metadata": {
        "id": "51e79f75401a38ab"
      },
      "source": [
        "# Improving Post-Retrieval Processes\n",
        "\n",
        "![](https://d27l3jncscxhbx.cloudfront.net/lib/media/img-20240427234647805.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb1fb467e9113c82",
      "metadata": {
        "id": "fb1fb467e9113c82"
      },
      "source": [
        "## Filtering by Score Threshold\n",
        "- This is a process of filtering out the chunks based on the similarity score.\n",
        "- We can set threshold for filtering.\n",
        "- Filtering removes noise and redundant information and passes only relevant context to LLM, which improves generation quality.\n",
        "\n",
        "For this demonstration, we will use the Naive Retriever.\n",
        "We recreate the vectordb here again, so we won't have to scroll up and refer to the earlier code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccc4481b",
      "metadata": {
        "id": "ccc4481b"
      },
      "outputs": [],
      "source": [
        "# Reset the collection in the vector store to have a clean slate\n",
        "vectordb.reset_collection()\n",
        "vectordb._collection.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecd20941",
      "metadata": {
        "id": "ecd20941"
      },
      "outputs": [],
      "source": [
        "# list of filenames to load\n",
        "filename_list = [\n",
        "    '2. Key Parameters for LLMs.txt',\n",
        "    '3. LLMs and Hallucinations.txt',\n",
        "    '4. Prompting Techniques for Builders.txt',\n",
        "]\n",
        "\n",
        "# load the documents\n",
        "list_of_documents_loaded = []\n",
        "for filename in filename_list:\n",
        "    try:\n",
        "        # try to load the document\n",
        "        markdown_path = os.path.join('notes', filename)\n",
        "        loader = TextLoader(markdown_path)\n",
        "\n",
        "        # load() returns a list of Document objects\n",
        "        data = loader.load()\n",
        "        # use extend() to add to the list_of_documents_loaded\n",
        "        list_of_documents_loaded.extend(data)\n",
        "        print(f\"Loaded {filename}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # if there is an error loading the document, print the error and continue to the next document\n",
        "        print(f\"Error loading {filename}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(\"Total documents loaded:\", len(list_of_documents_loaded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e94517fc",
      "metadata": {
        "id": "e94517fc"
      },
      "outputs": [],
      "source": [
        "# While our document is not too long, we can still split it into smaller chunks\n",
        "# This is to ensure that we can process the document in smaller chunks\n",
        "# This is especially useful for long documents that may exceed the token limit\n",
        "# or to keep the chunks smaller, so each chunk is more focused\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# In this case, we intentionally set the chunk_size to 1100 tokens, to have the smallest document (document 2) intact\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1100, chunk_overlap=10, length_function=count_tokens)\n",
        "\n",
        "# Split the documents into smaller chunks\n",
        "splitted_documents = text_splitter.split_documents(list_of_documents_loaded)\n",
        "\n",
        "# Print the number of documents after splitting\n",
        "print(f\"Number of documents after splitting: {len(splitted_documents)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c1539ce",
      "metadata": {
        "id": "3c1539ce"
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "# Create the vector database\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=splitted_documents,\n",
        "    embedding=embeddings_model,\n",
        "    collection_name=\"splitter_threshold\", # one database can have multiple collections\n",
        "    persist_directory=\"./vector_db\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0f56c1e",
      "metadata": {
        "id": "a0f56c1e"
      },
      "outputs": [],
      "source": [
        "# Create the RAG pipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "\n",
        "# ⚠️⚠️⚠️ This is the key step\n",
        "# We can set the threshold for the retriever, this is the minimum similarity score for the retrieved documents\n",
        "retriever_w_threshold = vectordb.as_retriever(\n",
        "        search_type=\"similarity_score_threshold\",\n",
        "        # There is no universal threshold, it depends on the use case\n",
        "        search_kwargs={'score_threshold': 0.20}\n",
        "    )\n",
        "# The `llm` is defined earlier in the notebook (using GPT-4o-mini)\n",
        "rag_chain = RetrievalQA.from_llm(\n",
        "    retriever=retriever_w_threshold, llm=llm\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a8b690e",
      "metadata": {
        "id": "3a8b690e"
      },
      "outputs": [],
      "source": [
        "# Now we can use the RAG pipeline to ask questions\n",
        "# Let's ask a question that we know is in the documents\n",
        "llm_response = rag_chain.invoke('What is Temperature in LLMs?')\n",
        "print(llm_response['result'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70be8fff",
      "metadata": {
        "id": "70be8fff"
      },
      "outputs": [],
      "source": [
        "# Use the high-level retriever object to retrieve the relevant documents\n",
        "retriever_w_threshold.invoke('What is Temperature in LLMs?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9b809af",
      "metadata": {
        "id": "a9b809af"
      },
      "outputs": [],
      "source": [
        "# Use the low-level vectorstore to see the retrieved documents with the relevance scores\n",
        "# Note that the `score_threshold` does not affect the vectorstore, it only affects the retriever\n",
        "vectordb.similarity_search_with_relevance_scores('What is Temperature in LLMs?', k=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "496cc12e",
      "metadata": {
        "id": "496cc12e"
      },
      "source": [
        "> 💡**OPTIONAL: If you're keen to find out more parameters that can be used in `as_retriever()` method, click the link below:\n",
        "\n",
        "<details>\n",
        "<summary><font size=\"2\" color=\"darkgreen\"><b>👆🏼 Click to for find out)</b></font></summary>\n",
        "\n",
        "<small>\n",
        "The term np.max(logits) is subtracted from logits to avoid numerical instability that can occur when taking the exponential of large numbers, a common issue in machine learning computations.\n",
        "\n",
        "Here's why: the softmax function involves taking the exponential of each logit. If a logit is a large positive number, its exponential can be extremely large - so large that it exceeds the maximum representable number (overflow), leading to inf values. This can cause the softmax function to return incorrect results.\n",
        "\n",
        "By subtracting np.max(logits), we ensure that the maximum value in the logits array is 0, which means the largest possible output from the exponential function is 1. This effectively eliminates the possibility of overflow.\n",
        "\n",
        "Importantly, this operation doesn't change the output of the softmax function. That's because softmax is shift invariant, meaning that adding or subtracting a constant from each logit doesn't affect the output probabilities. This property allows us to subtract the maximum logit for numerical stability without changing the function's output.\n",
        "\n",
        "\n",
        "### **Documentation for `as_retriever()` method**\n",
        "\n",
        "- Signature: vectordb.as_retriever(**kwargs: 'Any') -> 'VectorStoreRetriever'\n",
        "\n",
        "- Docstring:\n",
        "    - Return VectorStoreRetriever initialized from this VectorStore.\n",
        "\n",
        "- Args:\n",
        "    - **kwargs: Keyword arguments to pass to the search function.\n",
        "        - Can include:\n",
        "        - `search_type` (Optional[str]): Defines the type of search that\n",
        "            the Retriever should perform.\n",
        "            Can be \"similarity\" (default), \"mmr\", or\n",
        "            \"similarity_score_threshold\".\n",
        "        - `search_kwargs` (Optional[Dict]): Keyword arguments to pass to the\n",
        "            search function. Can include things like:\n",
        "            - `k`: Amount of documents to return (Default: 4)\n",
        "            - `score_threshold`: Minimum relevance threshold for similarity_score_threshold\n",
        "            - `fetch_k`: Amount of documents to pass to MMR algorithm (Default: 20)\n",
        "            - `lambda_mult`: Diversity of results returned by MMR; 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
        "            - `filter`: Filter by document metadata\n",
        "\n",
        "- Returns:\n",
        "    VectorStoreRetriever: Retriever class for VectorStore.\n",
        "\n",
        "- Examples:\n",
        "\n",
        "```Python\n",
        "\n",
        "    # Retrieve more documents with higher diversity\n",
        "    # Useful if your dataset has many similar documents\n",
        "    docsearch.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs={'k': 6, 'lambda_mult': 0.25}\n",
        "    )\n",
        "\n",
        "    # Fetch more documents for the MMR algorithm to consider\n",
        "    # But only return the top 5\n",
        "    docsearch.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs={'k': 5, 'fetch_k': 50}\n",
        "    )\n",
        "\n",
        "    # Only retrieve documents that have a relevance score\n",
        "    # Above a certain threshold\n",
        "    docsearch.as_retriever(\n",
        "        search_type=\"similarity_score_threshold\",\n",
        "        search_kwargs={'score_threshold': 0.8}\n",
        "    )\n",
        "\n",
        "    # Only get the single most similar document from the dataset\n",
        "    docsearch.as_retriever(search_kwargs={'k': 1})\n",
        "\n",
        "    # Use a filter to only retrieve documents from a specific paper\n",
        "    docsearch.as_retriever(\n",
        "        search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}}\n",
        "    )\n",
        "```\n",
        "</small>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0be17a6c",
      "metadata": {
        "id": "0be17a6c"
      },
      "source": [
        "## Cross-encoder Re-ranking"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dea01aed",
      "metadata": {
        "id": "dea01aed"
      },
      "source": [
        "- If you wish to go through this part, you will need to create an account on https://dashboard.cohere.com/.\n",
        "- After creating the account, go to `API Keys` from the left panel, then copy your API key inder the `Trial Keys` section\n",
        "- The whole process from account registration to get the API Key should be about 3 minutes.\n",
        "-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00b22aa4",
      "metadata": {
        "id": "00b22aa4"
      },
      "outputs": [],
      "source": [
        "cohere_api_key = getpass(\"Enter your cohere key\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a9d52c0",
      "metadata": {
        "id": "2a9d52c0"
      },
      "outputs": [],
      "source": [
        "from langchain_cohere import CohereRerank\n",
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "\n",
        "os.environ[\"COHERE_API_KEY\"] = cohere_api_key\n",
        "\n",
        "compressor = CohereRerank(top_n=3, model='rerank-english-v3.0')\n",
        "\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=vectordb.as_retriever(),\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e451f41",
      "metadata": {
        "id": "9e451f41"
      },
      "outputs": [],
      "source": [
        "# Test and examine retrieved documents from the retriever\n",
        "query = \"What is Temperature in LLMs?\"\n",
        "docs = compression_retriever.invoke(query)\n",
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08bc4cea",
      "metadata": {
        "id": "08bc4cea"
      },
      "outputs": [],
      "source": [
        "logging.getLogger(\"langchain.retrievers.contextual_compression\").setLevel(logging.DEBUG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "190e69b3",
      "metadata": {
        "id": "190e69b3"
      },
      "outputs": [],
      "source": [
        "# Using the retriever in a Q&A Chain\n",
        "retriever_from_llm_naive = RetrievalQA.from_llm(\n",
        "    retriever=compression_retriever, llm=llm\n",
        ")\n",
        "retriever_from_llm_naive.invoke('What is Temperature in LLMs?')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0Mdic9WpSmRA",
      "metadata": {
        "id": "0Mdic9WpSmRA"
      },
      "source": [
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab44c053-4d05-4cc5-8739-bbc71d19420d",
      "metadata": {
        "id": "ab44c053-4d05-4cc5-8739-bbc71d19420d"
      },
      "source": [
        "# Evaluating RAG Output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wNauvAdnEIGs",
      "metadata": {
        "id": "wNauvAdnEIGs"
      },
      "source": [
        "- For this part, we will be using the `answer` from LLM generated previous on the `question`\n",
        "- \"What is Temperature in LLMs?\" to emulate the ground truth for the `question`.\n",
        "Ideally, it should be a human-created/curated response to the `question`.\n",
        "\n",
        "Answer Generated.\n",
        "\n",
        "```\n",
        "{\n",
        "    'query': 'What is Temperature in LLMs?',\n",
        "    'result': 'In the context of Large Language Models (LLMs), \"temperature\" is a parameter that controls the randomness of the model’s predictions. A high temperature setting makes the model more likely to produce varied and sometimes unexpected responses, while a low temperature results in more predictable and conservative outputs. Essentially, it adjusts the probability distribution of the next token being generated, influencing the diversity of the generated text. \\n\\nAt lower temperatures, the model\\'s predictions become more deterministic, favoring the most likely next token, while at higher temperatures, the probabilities are more evenly distributed, leading to more randomness and creativity in the generated text.'\n",
        "}\n",
        " ```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4956092e",
      "metadata": {
        "id": "4956092e"
      },
      "outputs": [],
      "source": [
        "ground_truth_emulated = \"\"\"In the context of Large Language Models (LLMs), \"temperature\" is a parameter that controls the randomness of the model’s predictions. A high temperature setting makes the model more likely to produce varied and sometimes unexpected responses, while a low temperature results in more predictable and conservative outputs. Essentially, it adjusts the probability distribution of the next token being generated, influencing the diversity of the generated text. \\n\\nAt lower temperatures, the model\\'s predictions become more deterministic, favoring the most likely next token, while at higher temperatures, the probabilities are more evenly distributed, leading to more randomness and creativity in the generated text.'2. Key Parameters for LLMs.md\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d88cdce3d3782ee",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-28T12:53:13.556984Z",
          "start_time": "2024-04-28T12:53:13.554660Z"
        },
        "id": "5d88cdce3d3782ee"
      },
      "outputs": [],
      "source": [
        "\n",
        "from datasets import Dataset\n",
        "import os\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import faithfulness, context_recall, context_precision, answer_relevancy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rHuOQYmnTeML",
      "metadata": {
        "id": "rHuOQYmnTeML"
      },
      "source": [
        "\n",
        " For the evaluation, we will need the retrieved document(s) that passed to the LLM which resulted the LLM's answer.\n",
        " Below is how we get it from the `docs` variable from previos example\n",
        "\n",
        " ```Python\n",
        "query = \"What is Temperature in LLMs?\"\n",
        "\n",
        "# To retrieve relevant documents\n",
        "docs = compression_retriever.invoke(query)\n",
        "docs\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c4a678f",
      "metadata": {
        "id": "4c4a678f"
      },
      "outputs": [],
      "source": [
        "# Test and examine retrieved documents from the retriever\n",
        "# query = \"What is Temperature in LLMs?\"\n",
        "# docs = compression_retriever.invoke(query)\n",
        "# docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "O48yQGVaTESR",
      "metadata": {
        "id": "O48yQGVaTESR"
      },
      "outputs": [],
      "source": [
        "# Add only the `page_content` into the new list object\n",
        "list_of_contexts = []\n",
        "for doc in docs:\n",
        "  list_of_contexts.append(doc.page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd5b8149",
      "metadata": {
        "id": "fd5b8149"
      },
      "outputs": [],
      "source": [
        "\n",
        "data_samples = {\n",
        "    'question': ['Why do LLMs have problem with mathematical calculations?'],\n",
        "    'answer': ['LLMs may have problems with mathematical calculations because they are not specifically designed or trained to perform mathematical computations. Their primary function is to generate human-like text based on the patterns and data they have been trained on. Mathematical calculations require a different kind of processing and logic that is not the main focus of LLMs.'],\n",
        "    'contexts' : [list_of_contexts],\n",
        "    'ground_truth': [\"\"\"LLMs may have problems with mathematical calculations because they are not specifically designed or trained to perform mathematical computations. Their primary function is to generate human-like text based on the patterns and data they have been trained on. Mathematical calculations require a different kind of processing and logic that is not the main focus of LLMs.\"\"\"]\n",
        "}\n",
        "\n",
        "dataset = Dataset.from_dict(data_samples)\n",
        "\n",
        "score = evaluate(dataset,metrics=[faithfulness, context_recall, context_precision, answer_relevancy])\n",
        "score.to_pandas()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67bc3160",
      "metadata": {
        "id": "67bc3160"
      },
      "source": [
        "> ⚠️ Take note that some metrices not require the \"ground_truth\" field to be present in the dataset.\n",
        "> for more details, please refer to the various metrics in the `ragas` library [documentation on metrices](https://docs.ragas.io/en/latest/concepts/metrics/index.html#ragas-metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d14d924",
      "metadata": {
        "id": "7d14d924"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "aicamp-py3.12",
      "language": "python",
      "name": "aicamp-py3.12"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}