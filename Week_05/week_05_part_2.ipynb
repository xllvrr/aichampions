{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6203758e-73eb-4e7b-ad76-d41d604345c6",
   "metadata": {
    "id": "6203758e-73eb-4e7b-ad76-d41d604345c6"
   },
   "source": [
    "---\n",
    "---\n",
    "# Notebook: [ Week #05: Build Your RAG Pipeline with Enhanced Retrieval]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jvRuSC8AVIPw",
   "metadata": {
    "id": "jvRuSC8AVIPw"
   },
   "source": [
    "The challenges in this notebook are to implement at least:\n",
    "\n",
    "- 1 x Technique from **Pre-Retrieval Processes**\n",
    "- 1 x Technique from **Retrieval Processes**\n",
    "- 1 x Technique from **Post-Retrieval Processes**\n",
    "\n",
    "---\n",
    "\n",
    "Note:\n",
    "- You may want to challenge yourself to implement those techniques that are covered in our **Course Notes**, but **NOT in the walkthrough of the notebook**.\n",
    "- You can create as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496cd6c8-1d46-4f5c-9cf6-d6369cc52c1b",
   "metadata": {
    "id": "496cd6c8-1d46-4f5c-9cf6-d6369cc52c1b"
   },
   "source": [
    "## Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37b18427-b945-4641-a0b7-7ca5427226c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T12:00:24.632121Z",
     "start_time": "2024-04-28T12:00:20.490138Z"
    },
    "id": "37b18427-b945-4641-a0b7-7ca5427226c0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from getpass import getpass\n",
    "\n",
    "with open('../openai_key', 'r') as file:\n",
    "    API_KEY = file.read().rstrip()\n",
    "\n",
    "client = OpenAI(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d07baa",
   "metadata": {
    "id": "c4d07baa"
   },
   "source": [
    "---\n",
    "\n",
    "## Helper Functions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b03eda8",
   "metadata": {
    "id": "2b03eda8"
   },
   "source": [
    "### Function for Generating Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32f1fb11",
   "metadata": {
    "id": "32f1fb11"
   },
   "outputs": [],
   "source": [
    "def get_embedding(input, model='text-embedding-3-small'):\n",
    "    response = client.embeddings.create(\n",
    "        input=input,\n",
    "        model=model\n",
    "    )\n",
    "    return [x.embedding for x in response.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3435d399",
   "metadata": {
    "id": "3435d399"
   },
   "source": [
    "### Function for Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce3e807b",
   "metadata": {
    "id": "ce3e807b"
   },
   "outputs": [],
   "source": [
    "# This is the \"Updated\" helper function for calling LLM\n",
    "def get_completion(prompt, model=\"gpt-4o-mini\", temperature=0, top_p=1.0, max_tokens=256, n=1, json_output=False):\n",
    "    if json_output == True:\n",
    "      output_json_structure = {\"type\": \"json_object\"}\n",
    "    else:\n",
    "      output_json_structure = None\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create( #originally was openai.chat.completions\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=max_tokens,\n",
    "        n=1,\n",
    "        response_format=output_json_structure,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "614c7877",
   "metadata": {
    "id": "614c7877"
   },
   "outputs": [],
   "source": [
    "# This a \"modified\" helper function that we will discuss in this session\n",
    "# Note that this function directly take in \"messages\" as the parameter.\n",
    "def get_completion_by_messages(messages, model=\"gpt-4o-mini\", temperature=0, top_p=1.0, max_tokens=1024, n=1):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=max_tokens,\n",
    "        n=1\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a503547-3c2e-4b63-82a4-33e6da38e7a4",
   "metadata": {
    "id": "2a503547-3c2e-4b63-82a4-33e6da38e7a4"
   },
   "source": [
    "### Functions for Token Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7210eb23-50f8-4681-84a2-493f4708501a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T12:00:27.880949Z",
     "start_time": "2024-04-28T12:00:27.870060Z"
    },
    "id": "7210eb23-50f8-4681-84a2-493f4708501a"
   },
   "outputs": [],
   "source": [
    "# This function is for calculating the tokens given the \"message\"\n",
    "# ⚠️ This is simplified implementation that is good enough for a rough estimation\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "def count_tokens(text):\n",
    "    encoding = tiktoken.encoding_for_model('gpt-4o-mini')\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def count_tokens_from_message_rough(messages):\n",
    "    encoding = tiktoken.encoding_for_model('gpt-4o-mini')\n",
    "    value = ' '.join([x.get('content') for x in messages])\n",
    "    return len(encoding.encode(value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43375ba7e5dba945",
   "metadata": {
    "id": "43375ba7e5dba945"
   },
   "source": [
    "## Setting up Credentials & Common Components for LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2039d84463a85a43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T12:00:28.844113Z",
     "start_time": "2024-04-28T12:00:28.841045Z"
    },
    "id": "2039d84463a85a43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "os.environ[\"OPENAI_API_KEY\"] = API_KEY\n",
    "\n",
    "# embedding model that we will use for the session\n",
    "embeddings_model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "\n",
    "# llm to be used in RAG pipeplines in this notebook\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ea3f59",
   "metadata": {
    "id": "23ea3f59"
   },
   "source": [
    "---\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11f82d1",
   "metadata": {
    "id": "d11f82d1"
   },
   "source": [
    "**\\[ Overview of Steps in RAG \\]**\n",
    "\n",
    "- 1. **Document Loading**\n",
    "\t- In this initial step, relevant documents are ingested and prepared for further processing. This process typically occurs offline.\n",
    "- 2. **Splitting & Chunking**\n",
    "\t- The text from the documents is split into smaller chunks or segments.\n",
    "\t- These chunks serve as the building blocks for subsequent stages.\n",
    "- 3. **Storage**\n",
    "\t- The embeddings (vector representations) of these chunks are created and stored in a vector store.\n",
    "\t- These embeddings capture the semantic meaning of the text.\n",
    "- 4. **Retrieval**\n",
    "\t- When an online query arrives, the system retrieves relevant chunks from the vector store based on the query.\n",
    "\t- This retrieval step ensures that the system identifies the most pertinent information.\n",
    "- 5. **Output**\n",
    "\t- Finally, the retrieved chunks are used to generate a coherent response.\n",
    "\t- This output can be in the form of natural language text, summaries, or other relevant content.\n",
    "\n",
    "![](https://abc-notes.data.tech.gov.sg/resources/img/topic-4-rag-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509e2a85",
   "metadata": {
    "id": "509e2a85"
   },
   "source": [
    "# Setting Up the Common Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb607c36e1f155e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T12:00:31.142961Z",
     "start_time": "2024-04-28T12:00:30.800805Z"
    },
    "id": "cb607c36e1f155e"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l8uk8SuLWJ4i",
   "metadata": {
    "id": "l8uk8SuLWJ4i"
   },
   "source": [
    "## Download the notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f172c36",
   "metadata": {
    "id": "2f172c36"
   },
   "outputs": [],
   "source": [
    "# Download and unzip into local folder\n",
    "url = \"https://abc-notes.data.tech.gov.sg/resources/data/notes_rag.zip\"\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "response = requests.get(url)\n",
    "z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "\n",
    "# Take note that the files are unzipped into a folder\n",
    "z.extractall('./notes_rag')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c59df1",
   "metadata": {
    "id": "27c59df1"
   },
   "source": [
    "## Document Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ef5e533",
   "metadata": {
    "id": "3ef5e533"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e35df19",
   "metadata": {
    "id": "6e35df19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2. Key Parameters for LLMs.txt\n",
      "Loaded 3. LLMs and Hallucinations.txt\n",
      "Loaded 4. Prompting Techniques for Builders.txt\n",
      "Total documents loaded: 3\n"
     ]
    }
   ],
   "source": [
    "# list of filenames to load\n",
    "filename_list = [\n",
    "    '2. Key Parameters for LLMs.txt',\n",
    "    '3. LLMs and Hallucinations.txt',\n",
    "    '4. Prompting Techniques for Builders.txt',\n",
    "]\n",
    "\n",
    "# load the documents\n",
    "list_of_documents_loaded = []\n",
    "for filename in filename_list:\n",
    "    try:\n",
    "        # try to load the document\n",
    "        markdown_path = os.path.join('notes', filename)\n",
    "        loader = TextLoader(markdown_path)\n",
    "\n",
    "        # load() returns a list of Document objects\n",
    "        data = loader.load()\n",
    "        # use extend() to add to the list_of_documents_loaded\n",
    "        list_of_documents_loaded.extend(data)\n",
    "        print(f\"Loaded {filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # if there is an error loading the document, print the error and continue to the next document\n",
    "        print(f\"Error loading {filename}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"Total documents loaded:\", len(list_of_documents_loaded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaafe40a90410ca",
   "metadata": {
    "id": "aaafe40a90410ca"
   },
   "source": [
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rU7yeUllWp5H",
   "metadata": {
    "id": "rU7yeUllWp5H"
   },
   "source": [
    "# Technique(s) for Improving Pre-Retrieval Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2791e44-261d-4864-b779-ac82a087692b",
   "metadata": {},
   "source": [
    "## Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cRuS3YQtWjmB",
   "metadata": {
    "id": "cRuS3YQtWjmB"
   },
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "# Create the text splitter\n",
    "text_splitter = SemanticChunker(embeddings_model)\n",
    "\n",
    "# Split the documents into smaller chunks\n",
    "splitted_documents = text_splitter.split_documents(list_of_documents_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11820fb-3d36-46bb-9bdc-c303adab0ab6",
   "metadata": {},
   "source": [
    "## Multi Query Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd48e6d1-27d3-4394-9c63-4b148c4842a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "# Refer to LangChain documentation to find which loggers to set\n",
    "# Different LangChain Classes/Modules have different loggers to set\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df5bb6b2-34a9-4bbc-82ed-c80e9bbf255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Create the vector database\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splitted_documents,\n",
    "    embedding=embeddings_model,\n",
    "    collection_name=\"naive_splitter\", # one database can have multiple collections\n",
    "    persist_directory=\"./vector_db\"\n",
    ")\n",
    "\n",
    "# Create the multiquery retriever\n",
    "retriever_multiquery = MultiQueryRetriever.from_llm(\n",
    "  retriever=vectordb.as_retriever(), llm=llm,\n",
    ")\n",
    "\n",
    "# Create the multiquery pipeline\n",
    "qa_chain_multiquery= RetrievalQA.from_llm(\n",
    "    retriever=retriever_multiquery, llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b0d4e88-631f-462c-8f46-ad815fff5e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['What role does temperature play in the functioning of large language models (LLMs)?', 'How does the concept of temperature affect the output of LLMs during text generation?', 'Can you explain how temperature influences the behavior of large language models?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is temperature in LLMs?',\n",
       " 'result': 'In the context of Large Language Models (LLMs), \"temperature\" refers to a parameter that controls the randomness of the model’s predictions. A high temperature setting makes the model more likely to produce varied and sometimes unexpected responses, while a low temperature results in more predictable and conservative outputs. Essentially, it adjusts the probability distribution of the next token being generated, influencing the diversity of the generated text.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the multiquery pipeline\n",
    "qa_chain_multiquery.invoke(\"What is temperature in LLMs?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Isg7Z8tTW02-",
   "metadata": {
    "id": "Isg7Z8tTW02-"
   },
   "source": [
    "---\n",
    "---\n",
    "<br>\n",
    "\n",
    "# Technique(s) for Improving Retrieval Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2983c1e7-354c-4795-a8ac-9de60012d605",
   "metadata": {
    "id": "OsXN4MYWW5vP"
   },
   "source": [
    "## Parent-Child Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6be8b84c-359f-467c-b3d5-b86553e4623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "# This text splitter is used to create the parent documents\n",
    "parent_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n# \"], chunk_size=4000, length_function=count_tokens)\n",
    "\n",
    "# This text splitter is used to create the child documents\n",
    "# It should create documents smaller than the parent\n",
    "child_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n## \"], chunk_size=1250, length_function=count_tokens)\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectordb = Chroma(collection_name=\"parent_child\", embedding_function=embeddings_model)\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "\n",
    "# Specificy a Retriever\n",
    "parentchildretriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectordb,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    "    search_kwargs={'k': 5}\n",
    ")\n",
    "\n",
    "# The splitting & embeddings happen\n",
    "retriever.add_documents(list_of_documents_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71584fe3-e547-4f62-8982-ae79f42eec55",
   "metadata": {},
   "source": [
    "## Combining with Pre-Retrieval Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "513d8908-6e3b-4ccc-8d2a-a4f68f611153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the multiquery retriever\n",
    "retriever_multiquery = MultiQueryRetriever.from_llm(\n",
    "  retriever=parentchildretriever, llm=llm,\n",
    ")\n",
    "\n",
    "# Create the multiquery pipeline\n",
    "qa_chain_multiquery= RetrievalQA.from_llm(\n",
    "    retriever=retriever_multiquery, llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "970e460c-6bb5-4740-ab67-f04180f998c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['What role does temperature play in the functioning of large language models (LLMs)?', 'How does the concept of temperature affect the output generation in LLMs?', 'Can you explain how temperature influences the behavior of large language models?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is temperature in LLMs?',\n",
       " 'result': 'In the context of large language models (LLMs), \"temperature\" is a parameter that controls the randomness of the model\\'s output during text generation. A lower temperature (e.g., close to 0) makes the model\\'s predictions more deterministic and focused, often resulting in more coherent and sensible text. A higher temperature (e.g., above 1) increases randomness, allowing for more diverse and creative outputs, but it may also lead to less coherent or more erratic responses. Adjusting the temperature helps balance between creativity and coherence in generated text.'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain_multiquery.invoke(\"What is temperature in LLMs?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UHX6g9qFW7ES",
   "metadata": {
    "id": "UHX6g9qFW7ES"
   },
   "source": [
    "---\n",
    "---\n",
    "<br>\n",
    "\n",
    "# Technique(s) for Improving Post-retrieval Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TrMdCYacW-jd",
   "metadata": {
    "id": "TrMdCYacW-jd"
   },
   "outputs": [],
   "source": [
    "# Set a threshold score and dovetail with the previous techniques\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "# This text splitter is used to create the parent documents\n",
    "parent_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n# \"], chunk_size=4000, length_function=count_tokens)\n",
    "\n",
    "# This text splitter is used to create the child documents\n",
    "# It should create documents smaller than the parent\n",
    "child_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n## \"], chunk_size=1250, length_function=count_tokens)\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectordb = Chroma(collection_name=\"parent_child\", embedding_function=embeddings_model)\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "\n",
    "# Specificy a Retriever\n",
    "parentchildretriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectordb,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    "    search_kwargs={'k': 5, \"score_threshold\": 0.2 ## this is the new addition}\n",
    ")\n",
    "\n",
    "# The splitting & embeddings happen\n",
    "retriever.add_documents(list_of_documents_loaded)\n",
    "\n",
    "# Create the multiquery retriever\n",
    "retriever_multiquery = MultiQueryRetriever.from_llm(\n",
    "  retriever=parentchildretriever, llm=llm,\n",
    ")\n",
    "\n",
    "# Create the multiquery pipeline\n",
    "qa_chain_multiquery= RetrievalQA.from_llm(\n",
    "    retriever=retriever_multiquery, llm=llm\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
